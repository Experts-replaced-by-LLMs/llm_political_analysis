{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llm_political_analysis.modules.analyze import bulk_analyze_text_few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/plaintext/1 Test - POR1999 Soc Dem.txt',\n",
       " '../data/plaintext/1 Test - BUL 2014 Attack.txt',\n",
       " '../data/plaintext/1 Test - NOR 1989 Conservative txt.txt',\n",
       " '../data/plaintext/1 Test - FR 2002 FN txt.txt',\n",
       " '../data/plaintext/1 Test - SLO 2006 Xtian Dem Movement.txt',\n",
       " '../data/plaintext/1 Test - CRO 2003 Democratic Union.txt',\n",
       " '../data/plaintext/1 Test - POL 2019 Civic Coalition.txt',\n",
       " '../data/plaintext/1 Test - SWE 2010 Moderate.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist = os.listdir('../data/plaintext/')\n",
    "test_files = ['../data/plaintext/'+file for file in filelist if 'Test' in file]\n",
    "test_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['gpt-4o', 'gemini-1.5-pro-001', 'claude-3-5-sonnet-20240620']\n",
    "issue_list = ['european_union', 'taxation', 'lifestyle', 'immigration', 'environment', 'decentralization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file:  ../data/plaintext/1 Test - POR1999 Soc Dem.txt\n",
      "Summarized so far: 2 out of 2 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 5285 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "\n",
      "Error parsing response from model gemini-1.5-pro-001, retrying attempt 1\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "\n",
      "Error parsing response from model gemini-1.5-pro-001, retrying attempt 1\n",
      "\n",
      "Error parsing response from model gemini-1.5-pro-001, retrying attempt 1\n",
      "\n",
      "Error parsing response from model gemini-1.5-pro-001, retrying attempt 1\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - BUL 2014 Attack.txt\n",
      "Summarized so far: 1 out of 1 chunks\n",
      "Final summary length: 3612 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - NOR 1989 Conservative txt.txt\n",
      "Waiting for 44 seconds to avoid token limit\n",
      "Waiting for 43 seconds to avoid token limit\n",
      "Waiting for 43 seconds to avoid token limit\n",
      "Summarized so far: 4 out of 4 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 4995 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - FR 2002 FN txt.txt\n",
      "Waiting for 45 seconds to avoid token limit\n",
      "Waiting for 23 seconds to avoid token limit\n",
      "Summarized so far: 3 out of 3 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 5283 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - SLO 2006 Xtian Dem Movement.txt\n",
      "Summarized so far: 1 out of 1 chunks\n",
      "Final summary length: 4244 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - CRO 2003 Democratic Union.txt\n",
      "Waiting for 43 seconds to avoid token limit\n",
      "Summarized so far: 2 out of 2 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 5647 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "\n",
      "Error parsing response from model gemini-1.5-pro-001, retrying attempt 1\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - POL 2019 Civic Coalition.txt\n",
      "Waiting for 46 seconds to avoid token limit\n",
      "Summarized so far: 2 out of 2 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 5453 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  lifestyle\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  immigration\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  environment\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  decentralization\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "Analyzing file:  ../data/plaintext/1 Test - SWE 2010 Moderate.txt\n",
      "Waiting for 37 seconds to avoid token limit\n",
      "Summarized so far: 2 out of 2 chunks\n",
      "Combining summaries into one final summary\n",
      "Final summary length: 5381 characters \n",
      "\n",
      "-- Analyzing issue:  european_union\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n",
      "-- Analyzing issue:  taxation\n",
      "---- Analyzing with model:  gpt-4o\n",
      "---- Analyzing with model:  gemini-1.5-pro-001\n",
      "---- Analyzing with model:  claude-3-5-sonnet-20240620\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Bump up concurrency to 9 for faster processing with a Tier 2 OpenAI key, keep at 3 for Tier 1\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m bulk_analyze_text_few_shot(test_files, model_list, issue_list, results_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/results/test_set_few_shot_results.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, concurrency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[0;32m~/repos/llm_political_analysis/modules/analyze.py:284\u001b[0m, in \u001b[0;36mbulk_analyze_text_few_shot\u001b[0;34m(file_list, model_list, issue_list, results_file, summarize, parse_retries, max_retries, concurrency)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---- Analyzing with model: \u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[0;32m--> 284\u001b[0m     results \u001b[38;5;241m=\u001b[39m analyze_text_with_batch(prompts, model, parse_retries\u001b[38;5;241m=\u001b[39mparse_retries, max_retries\u001b[38;5;241m=\u001b[39mmax_retries, concurrency\u001b[38;5;241m=\u001b[39mconcurrency)\n\u001b[1;32m    285\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m    286\u001b[0m     results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124missue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m issue\n",
      "File \u001b[0;32m~/repos/llm_political_analysis/modules/analyze.py:151\u001b[0m, in \u001b[0;36manalyze_text_with_batch\u001b[0;34m(prompt_list, model, parse_retries, max_retries, concurrency)\u001b[0m\n\u001b[1;32m    149\u001b[0m prompt_batches \u001b[38;5;241m=\u001b[39m [prompt_list[i:i \u001b[38;5;241m+\u001b[39m concurrency] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompt_list), concurrency)]\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt_batch \u001b[38;5;129;01min\u001b[39;00m prompt_batches:\n\u001b[0;32m--> 151\u001b[0m     responses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mbatch(prompt_batch)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# This is hardcoded to expect a single score or NA in the response\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# If the desired response changes this will need to be updated\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Originally this handled a json response but that was removed to make\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# this more robust. \u001b[39;00m\n\u001b[1;32m    157\u001b[0m response_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/runnables/base.py:642\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[Output], [invoke(inputs[\u001b[38;5;241m0\u001b[39m], configs[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(configs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[Output], \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(invoke, inputs, configs)))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/runnables/config.py:499\u001b[0m, in \u001b[0;36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m contexts\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mrun(fn, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/runnables/base.py:635\u001b[0m, in \u001b[0;36mRunnable.batch.<locals>.invoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:248\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    245\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    247\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 248\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    249\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    250\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    251\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    252\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    253\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    254\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    255\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    256\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    257\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    258\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:677\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    671\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    675\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    676\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:534\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    533\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    536\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    538\u001b[0m ]\n\u001b[1;32m    539\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    523\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 524\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    525\u001b[0m                 m,\n\u001b[1;32m    526\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    527\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    528\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    529\u001b[0m             )\n\u001b[1;32m    530\u001b[0m         )\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:749\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 749\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    750\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    751\u001b[0m         )\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:525\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m         stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    522\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    523\u001b[0m         )\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m--> 525\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/resources/messages.py:904\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    903\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    907\u001b[0m             {\n\u001b[1;32m    908\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    909\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    910\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    911\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    912\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m    913\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    914\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m    915\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    916\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    917\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    918\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    919\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    920\u001b[0m             },\n\u001b[1;32m    921\u001b[0m             message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m    922\u001b[0m         ),\n\u001b[1;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    924\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    925\u001b[0m         ),\n\u001b[1;32m    926\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m    927\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    928\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[RawMessageStreamEvent],\n\u001b[1;32m    929\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1248\u001b[0m     )\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    932\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    933\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    934\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    935\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    936\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    937\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1014\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1013\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1015\u001b[0m         options,\n\u001b[1;32m   1016\u001b[0m         cast_to,\n\u001b[1;32m   1017\u001b[0m         retries,\n\u001b[1;32m   1018\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1019\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1020\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1066\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1067\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1068\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1014\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1013\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1015\u001b[0m         options,\n\u001b[1;32m   1016\u001b[0m         cast_to,\n\u001b[1;32m   1017\u001b[0m         retries,\n\u001b[1;32m   1018\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1019\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1020\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1066\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1067\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1068\u001b[0m )\n",
      "    \u001b[0;31m[... skipping similar frames: SyncAPIClient._request at line 1014 (4 times), SyncAPIClient._retry_request at line 1062 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1014\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1013\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m   1015\u001b[0m         options,\n\u001b[1;32m   1016\u001b[0m         cast_to,\n\u001b[1;32m   1017\u001b[0m         retries,\n\u001b[1;32m   1018\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1019\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1020\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1062\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1066\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1067\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1068\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm_poli/lib/python3.11/site-packages/anthropic/_base_client.py:1029\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1028\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1032\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1033\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1037\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of request tokens has exceeded your daily rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later. You may also contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
     ]
    }
   ],
   "source": [
    "# Bump up concurrency to 9 for faster processing with a Tier 2 OpenAI key, keep at 3 for Tier 1\n",
    "results = bulk_analyze_text_few_shot(test_files, model_list, issue_list, results_file='../data/results/test_set_few_shot_results.xlsx', concurrency=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_poli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
