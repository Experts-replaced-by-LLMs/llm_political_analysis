import numpy as np
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

from .prompts import get_prompts, policy_areas
from .summarize import summarize_text

def validate_score(score):
    return score in ['NA', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']

def analyze_text(messages, model, parse_retries=3, probabilities=False):
    """
    Analyzes the given text messages using the specified model.

    Parameters:
    - messages (list): A list of message objects representing the conversation.
    - model (str): The name or ID of the model to use for analysis.
    - parse_retries (int): The number of times to retry parsing the response. Defaults to 3.
    - probabilities (bool): Whether to include token probabilities in the response. Defaults to False. Only works with OpenAI models.

    Returns:
    - dict: A dictionary containing the analyzed text generated by the model.

    Checked models:
    - OpenAI models: ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4']
    - Claude models: ['claude-3-5-sonnet-20240620', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307']
    - Gemini models: ['gemini-1.5-pro-001']

    """
    # Tested models, this list should be updated as new models are added
    # Each service requires an API key to work, stored as an envorinment variable
    openai_model_list = ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4']
    claude_model_list = ['claude-3-5-sonnet-20240620', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307']
    gemini_model_list = ['gemini-1.5-pro-001']

    if model in openai_model_list:
        llm=ChatOpenAI(temperature=0, max_tokens=150, model_name=model)
    elif model in claude_model_list:
        llm=ChatAnthropic(temperature=0, max_tokens=150, model_name=model)
    elif model in gemini_model_list:
        llm=ChatGoogleGenerativeAI(temperature=0, max_tokens=150, model=model)
    else:
        print("You've selected a model that is not available.")
        print(f"Please select from the following models: {openai_model_list + claude_model_list + gemini_model_list}")

    # logprobs are only available for OpenAI models
    if probabilities and (model in openai_model_list):
        llm=llm.bind(logprobs=True)
    elif probabilities:
        print(f"Probabilities are not available for model {model}, please select a model from the following list: {openai_model_list}")
        
    # This is the core call to the model
    try: 
        response = llm.invoke(messages)
    except Exception as invocation_error:
        print(f'Error invoking model {model}: {invocation_error}')
        response_dict = {'score': 'NA', 'error_message':invocation_error}
        return response_dict
    
    # This is hardcoded to expect a single score or NA in the response
    # If the desired response changes this will need to be updated
    # Originally this handled a json response but that was removed to make
    # this more robust. 
    try:
        score = response.content.strip()
        if not validate_score(score):
            raise ValueError(f'Invalid score: {score}')
        response_dict = {'score': score, 'error_message': None}
    except Exception as original_error:
        attempt=1
        while attempt <= parse_retries:
            print(f'\nError parsing response from model {model}, retrying attempt {attempt}')
            try:
                response = llm.invoke(messages)
                score = response.content.strip()
                response_dict = {'score': score, 'error_message': None}
                break
            except:
                attempt += 1
        else:
            print(f'Retries failed with model {model}: {original_error}')
            response_dict = {'score': 'NA', 'error_message':response}
            return response_dict

    # Extract the probability of the score token from the response metadata
    if probabilities and (model in openai_model_list):
        try: 
            score = response_dict['score']
            response_meta_df = pd.DataFrame(response.response_metadata["logprobs"]["content"])
            score_metadata = response_meta_df[response_meta_df['token'] == str(score)].iloc[0]
            # note this is a little fragile, it will retrieve the probability of the first token in the response
            # that matches the score, which given the template "should" be the score itself, but it's not guaranteed
            prob = np.exp(score_metadata['logprob'])
            response_dict['prob'] = prob   
        except Exception as e:
            print(f'Error extracting probabilities from model {model}: {e}')
            response_dict['prob'] = 'NA'
        
    return response_dict


def batch_analyze_text(file_list, model_list, issue_list, probabilities=False, summary_model="gpt-4o"):
    """
    Analyzes a batch of text files using different models and prompts.

    Parameters:
    - file_list (list): A list of file paths containing the texts to analyze.
    - model_list (list): A list of model names to use for analysis. TODO give more info on what models are available
    - issue_list (list): A list of issue areas corresponding to each text file.
    - summary_model (str): The name or ID of the OpenAI model to use for summarization. Defaults to "gpt-4o".

    Returns:
    - list: A list of dictionaries containing the analyzed text generated by each model.

    """

    results = []
    # Loop through each file, issue area, model and prompt
    for file in file_list:
        print('Analyzing file: ', file)
        with open(file, 'r') as f:
            text = f.read()
        
        for issue in issue_list:
            print('-- Analyzing issue: ', issue)
            summary = summarize_text(text, policy_areas.get(issue, 'general policy issues'), model=summary_model)
            prompts = get_prompts(issue, summary)

            for model in model_list:
                print('---- Analyzing with model: ', model)

                for num, prompt in enumerate(prompts, start=1):
                    print(f'------ Analyzing with prompt: {num} out of {len(prompts)} ', end='\r')
                    result = analyze_text(prompt, model, probabilities=probabilities)
                    result['prompt'] = prompt
                    result['model'] = model
                    result['issue'] = issue
                    result['file'] = file
                    results.append(result)
                print('\n')

    return pd.DataFrame(results)[['file','issue','model','score','error_message','prompt']]