import json

import numpy as np
import pandas as pd
from langchain_openai import ChatOpenAI

from .prompts import get_prompts, policy_areas
from .summarize import summarize_text

def analyze_text(messages, model, json_retries=3, probabilities=False):
    """
    Analyzes the given text messages using the specified model.

    Parameters:
    - messages (list): A list of message objects representing the conversation.
    - model (str): The name or ID of the OpenAI model to use for analysis.
    - json_retries (int): The number of times to retry parsing the JSON response. Defaults to 3.
    - probabilities (bool): Whether to include token probabilities in the response. Defaults to False.

    Returns:
    - dict: A dictionary containing the analyzed text generated by the model.

    """

    llm=ChatOpenAI(temperature=0, max_tokens=150, model_name=model)

    if probabilities:
        llm=llm.bind(logprobs=True)
        
    # This is the core call to the model
    try: 
        response = llm.invoke(messages)
    except Exception as invocation_error:
        print(f'Error invoking model {model}: {invocation_error}')
        response_dict = {'score': 'NA', 'explanation':invocation_error}
        return response_dict
    
    # A common failure mode is to not return valid JSON, so we retry a few times
    try:
        response_dict = json.loads(response.content)
    except Exception as original_error:
        attempt=1
        while attempt <= json_retries:
            print(f'Error parsing response from model {model}, retrying attempt {attempt}')
            try:
                response = llm.invoke(messages)
                response_dict = json.loads(response.content)
                break
            except:
                attempt += 1
        else:
            print(f'Retries failed with model {model}: {original_error}')
            response_dict = {'score': 'NA', 'explanation':response}
            return response_dict

    # Extract the probability of the score token from the response metadata
    if probabilities:
        try: 
            score = response_dict['score']
            response_meta_df = pd.DataFrame(response.response_metadata["logprobs"]["content"])
            score_metadata = response_meta_df[response_meta_df['token'] == str(score)].iloc[0]
            # note this is a little fragile, it will retrieve the probability of the first token in the response
            # that matches the score, which given the template "should" be the score itself, but it's not guaranteed
            # a safer alternative would be to only ask for the score, and no explanation or json template. 
            prob = np.exp(score_metadata['logprob'])
            response_dict['prob'] = prob   
        except Exception as e:
            print(f'Error extracting probabilities from model {model}: {e}')
            response_dict['prob'] = 'NA'
        
    return response_dict


def batch_analyze_text(file_list, model_list, issue_list, probabilities=False, summary_model="gpt-4o"):
    """
    Analyzes a batch of text files using different models and prompts.

    Parameters:
    - file_list (list): A list of file paths containing the texts to analyze.
    - model_list (list): A list of model names to use for analysis. TODO give more info on what models are available
    - issue_list (list): A list of issue areas corresponding to each text file.
    - summary_model (str): The name or ID of the OpenAI model to use for summarization. Defaults to "gpt-4o".

    Returns:
    - list: A list of dictionaries containing the analyzed text generated by each model.

    """

    results = []
    # Loop through each file, issue area, model and prompt
    for file in file_list:
        print('Analyzing file: ', file)
        with open(file, 'r') as f:
            text = f.read()
        
        for issue in issue_list:
            print('-- Analyzing issue: ', issue)
            summary = summarize_text(text, policy_areas.get(issue, 'general policy issues'), model=summary_model)
            prompts = get_prompts(issue, summary)

            for model in model_list:
                print('---- Analyzing with model: ', model)

                for num, prompt in enumerate(prompts, start=1):
                    print(f'------ Analyzing with prompt: {num} out of {len(prompts)} ', end='\r')
                    result = analyze_text(prompt, model, probabilities=probabilities)
                    result['prompt'] = prompt
                    result['model'] = model
                    result['issue'] = issue
                    result['file'] = file
                    results.append(result)
                print('\n')

    return pd.DataFrame(results)[['file','issue','model','score','explanation','prompt']]